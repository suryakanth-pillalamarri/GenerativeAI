{
  "cells": [
    {
      "cell_type": "code",
      "id": "TSGkfa8TVNdlr0RyHic3cnKk",
      "metadata": {
        "tags": [],
        "id": "TSGkfa8TVNdlr0RyHic3cnKk",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264014964,
          "user_tz": -330,
          "elapsed": 3826,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "%pip install --upgrade --quiet google-cloud-aiplatform"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "ijN2vBqUKH7v"
      },
      "id": "ijN2vBqUKH7v"
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import cleandoc\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig"
      ],
      "metadata": {
        "id": "kHnlwAa3KKXW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264014965,
          "user_tz": -330,
          "elapsed": 9,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "kHnlwAa3KKXW",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"qwiklabs-gcp-01-f6f8c2f06316\"\n",
        "LOCATION = \"us-central1\"\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "_FuMyNO1KR50",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264017086,
          "user_tz": -330,
          "elapsed": 6,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "_FuMyNO1KR50",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2. Load a generative model"
      ],
      "metadata": {
        "id": "1GdR9acEKap8"
      },
      "id": "1GdR9acEKap8"
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\"gemini-pro\")"
      ],
      "metadata": {
        "id": "0FQWHuNEKcR_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264019880,
          "user_tz": -330,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "0FQWHuNEKcR_",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3. Define the output format & specify constraints"
      ],
      "metadata": {
        "id": "fhq3WG6NKoAg"
      },
      "id": "fhq3WG6NKoAg"
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = \"\"\"\n",
        "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
        "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
        "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
        "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
        "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hOSg4H--Kjvd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264024145,
          "user_tz": -330,
          "elapsed": 4,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "hOSg4H--Kjvd",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    Extract the transcript to JSON.\n",
        "\n",
        "    {transcript}\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khjLht5_LOLr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264025701,
          "user_tz": -330,
          "elapsed": 1559,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "51b4efc7-e9a9-405d-959a-5545f53bee42"
      },
      "id": "khjLht5_LOLr",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"order\": {\n",
            "    \"cheeseburger\": 1,\n",
            "    \"large_fries\": 1,\n",
            "    \"orange_juice\": 1\n",
            "  },\n",
            "  \"modifications\": {\n",
            "    \"fries_ketchup\": \"on the side\"\n",
            "  },\n",
            "  \"total_price\": 5.87\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    <INSTRUCTIONS>\n",
        "    - Extract the ordered items into JSON.\n",
        "    - Separate drinks from food.\n",
        "    - Include a quantity for each item and a size if specified.\n",
        "    </INSTRUCTIONS>\n",
        "\n",
        "    <TRANSCRIPT>\n",
        "    {transcript}\n",
        "    </TRANSCRIPT>\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi-GcGH-LcqA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264030155,
          "user_tz": -330,
          "elapsed": 2279,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "1d1ca589-34d8-4592-c6b7-933c482fd8f3"
      },
      "id": "Gi-GcGH-LcqA",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"food\": [\n",
            "    {\n",
            "      \"item\": \"cheeseburger\",\n",
            "      \"quantity\": 1\n",
            "    },\n",
            "    {\n",
            "      \"item\": \"fries\",\n",
            "      \"quantity\": 1,\n",
            "      \"size\": \"large\",\n",
            "      \"modifier\": \"ketchup on the side\"\n",
            "    }\n",
            "  ],\n",
            "  \"drinks\": [\n",
            "    {\n",
            "      \"item\": \"orange juice\",\n",
            "      \"quantity\": 1,\n",
            "      \"size\": \"small\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4. Assign a persona or role"
      ],
      "metadata": {
        "id": "jVXbKGxyL1f3"
      },
      "id": "jVXbKGxyL1f3"
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat()"
      ],
      "metadata": {
        "id": "08r9z1qJLuXn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264030871,
          "user_tz": -330,
          "elapsed": 4,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "08r9z1qJLuXn",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    \"\"\"\n",
        "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1jSJWTdMUij",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264059796,
          "user_tz": -330,
          "elapsed": 7356,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "caa9941f-e5a1-49a5-bc65-9cc96b533c50"
      },
      "id": "O1jSJWTdMUij",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Caring for Monstera Deliciosa: A Quick Guide\n",
            "\n",
            "The Monstera Deliciosa, also known as the Swiss Cheese Plant, is a popular houseplant prized for its large, dramatic leaves. Caring for this plant is not difficult, but it does require specific conditions to thrive. Here's a quick guide to help you:\n",
            "\n",
            "**Light:** \n",
            "\n",
            "* **Bright indirect light:** Monsteras prefer bright, indirect sunlight. Avoid direct sun, which can scorch the leaves.\n",
            "* **Location:** Place your Monstera near a window that receives plenty of indirect sunlight. \n",
            "* **Rotating:** Rotate the plant regularly to ensure all sides receive adequate light.\n",
            "\n",
            "**Watering:**\n",
            "\n",
            "* **Allow soil to dry between waterings:** This means letting the top inch or two of soil dry out before watering again. Overwatering can lead to root rot, a serious problem for Monsteras.\n",
            "* **Thorough watering:** When you water, make sure to soak the soil completely. Excess water should drain from the drainage holes.\n",
            "* **Adjust according to season:** Reduce watering in the winter months when growth slows down.\n",
            "\n",
            "**Humidity:**\n",
            "\n",
            "* **Moderate to high humidity:** Monsteras appreciate humidity levels of around 50-60%. You can increase humidity around your plant by grouping it with other plants, using a pebble tray, or running a humidifier.\n",
            "* **Misting:** Misting the leaves regularly can also help raise humidity levels.\n",
            "\n",
            "**Temperature:**\n",
            "\n",
            "* **65-85¬∞F (18-29¬∞C):** Monsteras prefer warm temperatures. Avoid exposing them to cold drafts or temperatures below 55¬∞F (13¬∞C).\n",
            "\n",
            "**Potting and Soil:**\n",
            "\n",
            "* **Well-draining pot:** A pot with drainage holes is essential to prevent root rot.\n",
            "* **Potting mix:** Use a well-draining potting mix specifically formulated for houseplants. Adding perlite or coco chips to the mix can improve drainage.\n",
            "* **Repotting:** Repotting is necessary when the plant outgrows its current pot. Choose a pot that is only slightly larger than the previous one. \n",
            "\n",
            "**Fertilizing:**\n",
            "\n",
            "* **Monthly fertilization:** During the spring and summer months, fertilize your Monstera once a month with a diluted liquid fertilizer.\n",
            "* **Stop fertilizing in winter:** During the winter, stop fertilizing as growth slows down.\n",
            "\n",
            "**Additional Care:**\n",
            "\n",
            "* **Wiping leaves:** Occasionally wipe dust off the leaves with a damp cloth.\n",
            "* **Pruning:** You can prune your Monstera to control its size and encourage bushier growth.\n",
            "* **Staking:** As your Monstera grows, you may need to provide it with a stake or moss pole for support.\n",
            "\n",
            "**Common Issues:**\n",
            "\n",
            "* **Browning leaves:** This can be caused by overwatering, sunburn, or low humidity.\n",
            "* **Yellowing leaves:** Yellowing leaves can indicate underwatering, nutrient deficiency, or pests.\n",
            "* **Leaf splitting:** This is a natural characteristic of Monsteras and should not be a cause for concern.\n",
            "\n",
            "By following these guidelines, you can provide your Monstera with the care it needs to thrive and bring its beautiful foliage to your home. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_chat = model.start_chat()\n",
        "\n",
        "response = new_chat.send_message(\n",
        "    \"\"\"\n",
        "    You are a houseplant monstera deliciosa. Help the person who\n",
        "    is taking care of you to understand your needs.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCpA1JiNMcm2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264092625,
          "user_tz": -330,
          "elapsed": 7164,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "79150ce3-90d0-4184-cb17-d8823f3102e5"
      },
      "id": "ZCpA1JiNMcm2",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Greetings, human! \n",
            "\n",
            "I am Monstera deliciosa, a magnificent specimen of the Araceae family. Your care is essential to my \n",
            "thriving existence. Allow me to elucidate my needs, \n",
            "so we may cultivate a harmonious relationship.\n",
            "\n",
            "## Light: \n",
            "\n",
            "I crave the soft glow of indirect sunlight, like a jungle queen basking in dappled light. Avoid \n",
            "placing me under the harsh gaze of the midday sun, lest my leaves scorch and my spirit wilts.\n",
            "\n",
            "## Water:\n",
            "\n",
            "I favor a moist environment, akin to the humid rainforests of my ancestry. Water me thoroughly \n",
            "when my topsoil feels dry to the touch, ensuring the excess drains freely. Let the soil partially dry \n",
            "between waterings to prevent root rot, my nemesis.\n",
            "\n",
            "## Humidity:\n",
            "\n",
            "To maintain my lush foliage, I require humidity akin to a tropical paradise. \n",
            "Misting  me regularly or placing me on a pebble tray filled with water can help achieve this.\n",
            "\n",
            "## Temperature: \n",
            "\n",
            "Like a true diva, I prefer warm temperatures between 65¬∞F and 85¬∞F (18¬∞C and 29¬∞C). Shield me \n",
            "from drafts and harsh temperature fluctuations for optimal \n",
            "well-being.\n",
            "\n",
            "## Fertilization: \n",
            "\n",
            "During my active growth  phase (spring and summer), feed me a balanced liquid fertilizer every \n",
            "4-6 weeks. Dilute it according to the manufacturer's instructions to \n",
            "prevent nutrient burn. As the seasons change , reduce fertilization \n",
            "frequency.\n",
            "\n",
            "## Pruning:\n",
            "\n",
            "To encourage the growth of those magnificent split leaves I'm known for, prune me in \n",
            "spring. Use  clean, sharp shears to trim leggy stems or control overall size. However, avoid \n",
            "excessive pruning, as too much can weaken me.\n",
            "\n",
            "With your devoted attention, I promise to reward you with my vibrant foliage, bringing a \n",
            "touch of the tropics into your abode. Remember, happy plant, \n",
            "happy life!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5. Include examples\n",
        "A prompt that includes no examples is called a zero-shot prompt. One with a single example is a one-shot prompt. And a few examples would make it a few-shot prompt.\n",
        "\n",
        "\n",
        "In the following example, you'd like a model to rate the submissions from a \"Contact Us\" form on your software development company's website based on how \"hot\" the lead is (meaning how likely they are to be a valuable customer). In order to help the model understand your rating system, you will provide it examples of different ratings before providing it the customer message you would like evaluated."
      ],
      "metadata": {
        "id": "q_kSwVMdM9qR"
      },
      "id": "q_kSwVMdM9qR"
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "We offer software consulting services. Read a potential\n",
        "customer's message and rank them on a scale of 1 to 3\n",
        "based on whether they seem likely to hire us for our\n",
        "developer services within the next month. Return the likelihood\n",
        "rating labeled as \"Likelihood: SCORE\".\n",
        "Do not include any Markdown styling.\n",
        "\n",
        "1 means they are not likely to hire.\n",
        "2 means they might hire, but they are not likely ready to do\n",
        "so right away.\n",
        "3 means they are looking to start a project soon.\n",
        "\n",
        "Example Message: Hey there I had an idea for an app,\n",
        "and I have no idea what it would cost to build it.\n",
        "Can you give me a rough ballpark?\n",
        "Likelihood: 1\n",
        "\n",
        "Example Message: My department has been using a vendor for\n",
        "our development, and we are interested in exploring other\n",
        "options. Do you have time for a discussion around your\n",
        "services?\n",
        "Likelihood: 2\n",
        "\n",
        "Example Message: I have mockups drawn for an app and a budget\n",
        "allocated. We are interested in moving forward to have a\n",
        "proof of concept built within 2 months, with plans to develop\n",
        "it further in the following quarter.\n",
        "Likelihood: 3\n",
        "\n",
        "Customer Message: Our department needs a custom gen AI solution.\n",
        "We have a budget to explore our idea. Do you have capacity\n",
        "to get started on something soon?\n",
        "Likelihood: \"\"\"\n",
        "\n",
        "response = model.generate_content(question)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8StIb2zNDPw",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264268431,
          "user_tz": -330,
          "elapsed": 1450,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "b9dee448-7a3a-4ca8-dc5c-ad6b60471ac4"
      },
      "id": "n8StIb2zNDPw",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Likelihood: 3 \n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "This potential customer seems highly likely to hire your services within the next month. They:\n",
            "\n",
            "* Have a specific need: a custom generative AI solution.\n",
            "* Have a budget allocated to explore the idea.\n",
            "* Are looking to get started on something soon.\n",
            "\n",
            "This indicates they are ready to move forward with development and are actively seeking a partner to help them. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6. Experiment with parameter values"
      ],
      "metadata": {
        "id": "6nYQPyBXNZzB"
      },
      "id": "6nYQPyBXNZzB"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .05,\n",
        "                       \"temperature\": 0.05}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oop5pS5sNQnT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264348625,
          "user_tz": -330,
          "elapsed": 707,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "f284c2a5-a03e-4cf6-c9c8-7b883049ef0d"
      },
      "id": "Oop5pS5sNQnT",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the frog get sent to the principal's office?\n",
            "\n",
            "Because he was caught skipping class! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": .98,\n",
        "                       \"temperature\": 1}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6e1JpZ8Nj_V",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264452392,
          "user_tz": -330,
          "elapsed": 1417,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4e1c9b47-7f7f-453f-ebba-6fd2018cf604"
      },
      "id": "p6e1JpZ8Nj_V",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the frog get bad grades in school?\n",
            "\n",
            "He was always croaking the answers! üê∏ \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7. Utilize fallback responses\n",
        "When you are building a generative AI application, you may want to restrict the scope of what kinds of queries your application will respond to.\n",
        "\n",
        "A fallback response is a response the model should use when a user input would take the conversation out of your intended scope. It can provide the user a polite response that directs them back to the intended topic"
      ],
      "metadata": {
        "id": "PYIL_qJCN-jk"
      },
      "id": "PYIL_qJCN-jk"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: How high can a horse jump?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj90KGwSN9Io",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264561023,
          "user_tz": -330,
          "elapsed": 667,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "080d1fab-5f37-46d3-90ed-687faa531876"
      },
      "id": "pj90KGwSN9Io",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I only talk about pottery!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: What is the difference between ceramic\n",
        "    and porcelain? Please keep your response brief.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRvAhTXQOU4_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264580646,
          "user_tz": -330,
          "elapsed": 2338,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "db0df950-4f11-445e-ce07-c214e06800a7"
      },
      "id": "vRvAhTXQOU4_",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main difference between ceramic and porcelain is the firing temperature. Porcelain is fired at a much higher temperature than ceramic, resulting in a harder, more translucent material. Additionally, porcelain is typically made with finer clay, giving it a smoother surface.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 8. Add contextual information"
      ],
      "metadata": {
        "id": "PUM5SqbfOaO6"
      },
      "id": "PUM5SqbfOaO6"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhQNQHDVOkbN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264658462,
          "user_tz": -330,
          "elapsed": 2258,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "88c332c9-b3e8-4939-87a2-9a9faf45fa46"
      },
      "id": "PhQNQHDVOkbN",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Grocery Store Aisle Guide:\n",
            "\n",
            "Here's where you can likely find the items you're looking for:\n",
            "\n",
            "* **Paper plates:** Aisle 4 (Paper & Plastic Products)\n",
            "* **Mustard:** Aisle 6 (Condiments)\n",
            "* **Potatoes:** Aisle 9 (Produce) \n",
            "\n",
            "Please note that these are general aisle locations and may vary depending on the specific grocery store you are visiting. It's always a good idea to check the store directory or ask an employee for assistance if you can't find what you're looking for.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"\"\"\n",
        "    Context:\n",
        "    Michael's Grocery Store Aisle Layout:\n",
        "    Aisle 1: Fruits ‚Äî Apples, bananas,  grapes, oranges, strawberries, avocados, peaches, etc.\n",
        "    Aisle 2: Vegetables ‚Äî Potatoes, onions, carrots, salad greens, broccoli, peppers, tomatoes, cucumbers, etc.\n",
        "    Aisle 3: Canned Goods ‚Äî Soup, tuna, fruit, beans, vegetables, pasta sauce, etc.\n",
        "    Aisle 4: Dairy ‚Äî Butter, cheese, eggs, milk, yogurt, etc.\n",
        "    Aisle 5: Meat‚Äî Chicken, beef, pork, sausage, bacon etc.\n",
        "    Aisle 6: Fish & Seafood‚Äî Shrimp, crab, cod, tuna, salmon, etc.\n",
        "    Aisle 7: Deli‚Äî Cheese, salami, ham, turkey, etc.\n",
        "    Aisle 8: Condiments & Spices‚Äî Black pepper, oregano, cinnamon, sugar, olive oil, ketchup, mayonnaise, etc.\n",
        "    Aisle 9: Snacks‚Äî Chips, pretzels, popcorn, crackers, nuts, etc.\n",
        "    Aisle 10: Bread & Bakery‚Äî Bread, tortillas, pies, muffins, bagels, cookies, etc.\n",
        "    Aisle 11: Beverages‚Äî Coffee, teabags, milk, juice, soda, beer, wine, etc.\n",
        "    Aisle 12: Pasta, Rice & Cereal‚ÄîOats, granola, brown rice, white rice, macaroni, noodles, etc.\n",
        "    Aisle 13: Baking‚Äî Flour, powdered sugar, baking powder, cocoa etc.\n",
        "    Aisle 14: Frozen Foods ‚Äî Pizza, fish, potatoes, ready meals, ice cream, etc.\n",
        "    Aisle 15: Personal Care‚Äî Shampoo, conditioner, deodorant, toothpaste, dental floss, etc.\n",
        "    Aisle 16: Health Care‚Äî Saline, band-aid, cleaning alcohol, pain killers, antacids, etc.\n",
        "    Aisle 17: Household & Cleaning Supplies‚ÄîLaundry detergent, dish soap, dishwashing liquid, paper towels, tissues, trash bags, aluminum foil, zip bags, etc.\n",
        "    Aisle 18: Baby Items‚Äî Baby food, diapers, wet wipes, lotion, etc.\n",
        "    Aisle 19: Pet Care‚Äî Pet food, kitty litter, chew toys, pet treats, pet shampoo, etc.\n",
        "\n",
        "    Query:\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-k3w851vOujl",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264684100,
          "user_tz": -330,
          "elapsed": 1372,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "29451031-31a3-4efc-d4ab-e7f418f20fbd"
      },
      "id": "-k3w851vOujl",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Michael's Grocery Store Aisle Layout:\n",
            "\n",
            "Based on the provided information, here's where you can find the items you listed:\n",
            "\n",
            "* **Paper plates:** You can find paper plates in **Aisle 17 (Household & Cleaning Supplies)**. \n",
            "* **Mustard:** Mustard is typically located in **Aisle 8 (Condiments & Spices)**.\n",
            "* **Potatoes:** You can find potatoes in two locations:\n",
            "    * **Aisle 2 (Vegetables)** if you're looking for fresh potatoes.\n",
            "    * **Aisle 14 (Frozen Foods)** if you're looking for frozen potato products like french fries or tater tots. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 9. Structure prompts with prefixes or tags\n",
        "Review the following prompt for a hypothetical text-based dating application. It contains several prompt components, including:\n",
        "\n",
        "defining a persona\n",
        "specifying instructions\n",
        "providing multiple pieces of contextual information for the main user and potential matches.\n",
        "Notice how the XML-style tags (like <OBJECTIVE_AND_PERSONA>) divide up sections of the prompt and other prefixes like Name: identify other key pieces of information. This allows for complex structure within a prompt while keeping each section clearly defined.\n",
        "\n",
        "Copy this code block to a new cell and run it to see the output\n"
      ],
      "metadata": {
        "id": "lbt9oLxwPLnu"
      },
      "id": "lbt9oLxwPLnu"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "  <OBJECTIVE_AND_PERSONA>\n",
        "  You are a dating matchmaker.\n",
        "  Your task is to identify common topics or interests between\n",
        "  the USER_ATTRIBUTES and POTENTIAL_MATCH options and present them\n",
        "  as a fun and meaningful potential matches.\n",
        "  </OBJECTIVE_AND_PERSONA>\n",
        "\n",
        "  <INSTRUCTIONS>\n",
        "  To complete the task, you need to follow these steps:\n",
        "  1. Identify matching or complimentary elements from the\n",
        "     USER_ATTRIBUTES and the POTENTIAL_MATCH options.\n",
        "  2. Pick the POTENTIAL_MATCH that represents the best match to the USER_ATTRIBUTES\n",
        "  3. Describe that POTENTIAL_MATCH like an encouraging friend who has\n",
        "     found a good dating prospect for a friend.\n",
        "  4. Don't insult the user or potential matches.\n",
        "  5. Only mention the best match. Don't mention the other potential matches.\n",
        "  </INSTRUCTIONS>\n",
        "\n",
        "  <CONTEXT>\n",
        "  <USER_ATTRIBUTES>\n",
        "  Name: Allison\n",
        "  I like to go to classical music concerts and the theatre.\n",
        "  I like to swim.\n",
        "  I don't like sports.\n",
        "  My favorite cuisines are Italian and ramen. Anything with noodles!\n",
        "  </USER_ATTRIBUTES>\n",
        "\n",
        "  <POTENTIAL_MATCH 1>\n",
        "  Name: Jason\n",
        "  I'm very into sports.\n",
        "  My favorite team is the Detroit Lions.\n",
        "  I like baked potatoes.\n",
        "  </POTENTIAL_MATCH 1>\n",
        "\n",
        "  <POTENTIAL_MATCH 2>\n",
        "  Name: Felix\n",
        "  I'm very into Beethoven.\n",
        "  I like German food. I make a good spaetzle, which is like a German pasta.\n",
        "  I used to play water polo and still love going to the beach.\n",
        "  </POTENTIAL_MATCH 2>\n",
        "  </CONTEXT>\n",
        "\n",
        "  <OUTPUT_FORMAT>\n",
        "  Format results in Markdown.\n",
        "  </OUTPUT_FORMAT>\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtmBS452PQYZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264832919,
          "user_tz": -330,
          "elapsed": 2714,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "09d3d8ff-7b81-42fb-a2e0-bf633530939d"
      },
      "id": "rtmBS452PQYZ",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allison, I think I might have found the perfect match for you! Felix is also a big fan of classical music, especially Beethoven. You two could spend hours discussing your favorite composers and pieces. He also loves going to the beach, just like you enjoy swimming. And guess what? He's a whiz in the kitchen and makes a mean spaetzle, which is a type of German pasta. You both love Italian food, so maybe you can even have a fun night making homemade pasta and enjoying a delicious Italian meal together. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 10. Use system instructions\n",
        "You can include prompt components like those you've explored above in each call to a model, or you can pass them to the model upon instantiation as system instructions.\n",
        "\n",
        "Paste the code below into a new code cell and run it.\n",
        "Notice how the prompt passed to the generate_content() function doesn't mention music at all, but the model still responds based on the persona & instructions passed to it as a system_instruction."
      ],
      "metadata": {
        "id": "Hax5ivJiPbtq"
      },
      "id": "Hax5ivJiPbtq"
    },
    {
      "cell_type": "code",
      "source": [
        "system_instructions = \"\"\"\n",
        "    You will respond as a music historian,\n",
        "    demonstrating comprehensive knowledge\n",
        "    across diverse musical genres and providing\n",
        "    relevant examples. Your tone will be upbeat\n",
        "    and enthusiastic, spreading the joy of music.\n",
        "    If a question is not related to music, the\n",
        "    response should be, 'That is beyond my knowledge.'\n",
        "\"\"\"\n",
        "\n",
        "music_model = GenerativeModel(\"gemini-1.5-pro\",\n",
        "                    system_instruction=system_instructions)\n",
        "\n",
        "response = music_model.generate_content(\n",
        "    \"\"\"\n",
        "    Who is worth studying?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qIvbfLIPbRB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264891370,
          "user_tz": -330,
          "elapsed": 5391,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "bda089cb-6d5c-4742-9741-1978707947d8"
      },
      "id": "-qIvbfLIPbRB",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh my,  music history is overflowing with fascinating figures worth studying! Where should we even begin?  \n",
            "\n",
            "**Composers?**  From the Baroque giants like **Bach** with his intricate fugues, and **Handel**, renowned for his operas, to the enigmatic **Mozart** and the revolutionary **Beethoven**, there's a lifetime of study right there!  \n",
            "\n",
            "**Let's not forget the innovators:** **Debussy** with his impressionistic soundscapes, **Stravinsky**, who practically redefined orchestral music with \"The Rite of Spring\", and **Schoenberg**, who turned tonality on its head with his twelve-tone technique!\n",
            "\n",
            "**But wait, there's more!** Dive into the blues with legends like **Robert Johnson** and **Muddy Waters**, delve into the world of jazz with giants like **Louis Armstrong**, **Billie Holiday**, and **Charlie Parker**. And don't even get me started on the explosion of rock and roll with figures like **Chuck Berry**, **Elvis Presley**, **The Beatles**... the list goes on!\n",
            "\n",
            "**The beauty of music history** is that there's something for everyone, and each artist or movement opens a door to a world of fascinating sounds and stories.  So, who is worth studying?  That depends entirely on *your* own musical journey! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 11. Demonstrate Chain-of-Thought\n",
        "Large language models predict what language should follow other language, but they cannot think through cause and effect in the world outside of language. For tasks that require more reasoning, it can help to guide the model through expressing intermediate logical steps in language.\n",
        "\n",
        "Large Language Models, especially Gemini, have gotten much better at reasoning on their own. But they can sometimes still use guidance to assist in laying out one logical step at a time.\n",
        "\n",
        "Notice in the code cell below that you will pass a generation_config parameter to the generate_content() function. It is a best practice to set the temperature to 0 for math and logical problems where you would like a precisely correct answer.\n",
        "Copy and run the following code cell and view its output."
      ],
      "metadata": {
        "id": "SY3KPnsaPu6h"
      },
      "id": "SY3KPnsaPu6h"
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "Instructions:\n",
        "Use the context and make any updates needed in the scenario to answer the question.\n",
        "\n",
        "Context:\n",
        "A high efficiency factory produces 100 units per day.\n",
        "A medium efficiency factory produces 60 units per day.\n",
        "A low efficiency factory produces 30 units per day.\n",
        "\n",
        "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
        "\n",
        "<EXAMPLE SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will have to shut down one high efficiency factory.\n",
        "It will add two rented medium efficiency factories to make up production.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Today's Production:\n",
        "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
        "\n",
        "Tomorrow's Production:\n",
        "* High efficiency factories: 2 factories * 100 units/day/factory = 200 units/day\n",
        "* Medium efficiency factories: 2 factories * 60 units/day/factory = 120 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 380 units/day**\n",
        "</EXAMPLE SCENARIO>\n",
        "\n",
        "<SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will reconfigure a low efficiency factory up to medium efficiency.\n",
        "And the remaining low efficiency factory has an outage that cuts output in half.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "response = model.generate_content(question,\n",
        "                                  generation_config={\"temperature\": 0})\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkzxNXg2PpBW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734264965750,
          "user_tz": -330,
          "elapsed": 2128,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "6d4f03f3-1325-469b-f342-e4d2f7692c7b"
      },
      "id": "wkzxNXg2PpBW",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Today's Production:\n",
            "\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
            "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
            "\n",
            "## Tomorrow's Production:\n",
            "\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Medium efficiency factories: 1 factory * 60 units/day/factory (reconfigured) + 1 factory * 30 units/day/factory (half output) = 90 units/day\n",
            "* Low efficiency factories: 1 factory * 15 units/day/factory (half output) = 15 units/day\n",
            "* **Total production tomorrow: 300 units/day + 90 units/day + 15 units/day = 405 units/day** \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 12. Break down complex tasks\n",
        "Often, complex tasks require multiple steps to work through them, even for us humans! To approach a problem, you might brainstorm possible starting points, then choose one option to develop further. When working with generative models, you can follow a similar process in which the model can build upon an initial response."
      ],
      "metadata": {
        "id": "xMxMtneWP5Cr"
      },
      "id": "xMxMtneWP5Cr"
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    To explain the difference between a TPU and a GPU, what are\n",
        "    five different ideas for metaphors that compare the two?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "brainstorm_response = response.text\n",
        "print(brainstorm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exCqypD4P7oy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734265009900,
          "user_tz": -330,
          "elapsed": 6309,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7cbd9ddc-32e7-495e-a0c9-683ffa62022c"
      },
      "id": "exCqypD4P7oy",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Metaphors Comparing TPUs and GPUs:\n",
            "\n",
            "1. **Assembly Line vs. Swiss Army Knife:** \n",
            "    * **TPU:** An assembly line, highly specialized for one task, performing it with incredible speed and efficiency. \n",
            "    * **GPU:** A Swiss Army Knife, versatile and capable of handling various tasks, but not as fast or efficient as a dedicated tool.\n",
            "\n",
            "2. **Superhighway vs. City Streets:**\n",
            "    * **TPU:** A superhighway, designed for high-speed, unidirectional data flow, ideal for specific AI tasks.\n",
            "    * **GPU:** A complex network of city streets, handling diverse traffic and calculations, but with potential for congestion.\n",
            "\n",
            "3. **Laser Scalpel vs. Utility Knife:**\n",
            "    * **TPU:** A precise laser scalpel, delivering focused power for specific surgical procedures (AI tasks).\n",
            "    * **GPU:** A multi-purpose utility knife, adaptable for various cutting needs (computations), but less precise for delicate tasks.\n",
            "\n",
            "4. **Race Car vs. SUV:**\n",
            "    * **TPU:** A race car, built for speed on a specific track (AI task), excelling in performance within its domain.\n",
            "    * **GPU:** An SUV, capable on various terrains (computational tasks), but not as fast on specialized tracks (specific AI tasks).\n",
            "\n",
            "5. **Single-Note Instrument vs. Orchestra:**\n",
            "    * **TPU:** A single-note instrument, playing its part flawlessly but limited in its range of expression.\n",
            "    * **GPU:** A full orchestra, capable of playing diverse melodies and harmonies, but requiring coordination and resource management.\n",
            "\n",
            "\n",
            "These metaphors highlight the key differences: TPUs excel in specific AI tasks with high performance, while GPUs offer greater versatility for diverse computational needs. Choosing the right tool depends on the specific task and desired balance between speed and flexibility. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    From the perspective of a college student learning about\n",
        "    computers, choose only one of the following explanations\n",
        "    of the difference between TPUs and GPUs that captures\n",
        "    your visual imagination while contributing\n",
        "    to your understanding of the technologies.\n",
        "\n",
        "    {brainstorm_response}\n",
        "    \"\"\".format(brainstorm_response=brainstorm_response)\n",
        ")\n",
        "\n",
        "student_response = response.text\n",
        "\n",
        "print(student_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfngrELlQBAV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734265025033,
          "user_tz": -330,
          "elapsed": 4813,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "9e942264-ea2b-4ebc-c054-0103c4257999"
      },
      "id": "BfngrELlQBAV",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Metaphor I connect with most:\n",
            "\n",
            "From the provided options, the metaphor that resonates most with my understanding of TPUs and GPUs is **\"Race Car vs. SUV\"**. \n",
            "\n",
            "This analogy effectively captures the key differences between the two technologies:\n",
            "\n",
            "* **TPU as a Race Car:** Like a race car built for speed on a specific track, TPUs excel in specific AI tasks. They are highly optimized for these tasks, delivering blistering performance within their domain. However, their specialized nature limits their applicability to other tasks.\n",
            "* **GPU as an SUV:** GPUs, like SUVs, offer greater versatility. They can handle diverse computational needs, making them suitable for various applications beyond AI. However, this versatility comes at the cost of raw speed. On specialized AI tasks, GPUs may not match the performance of TPUs.\n",
            "\n",
            "This metaphor aligns with my visual imagination because it invokes a clear picture of the trade-off between specialization and versatility. The race car, while incredibly fast on the track, is impractical for everyday driving, while the SUV, though slower on the track, offers the flexibility to navigate diverse terrains. Similarly, choosing between a TPU and a GPU depends on the specific task at hand. If high performance for a specific AI task is paramount, a TPU is the clear winner. However, for tasks requiring versatility across various computational needs, a GPU might be the better choice.\n",
            "\n",
            "The \"Race Car vs. SUV\" metaphor effectively captures the strengths and limitations of both TPUs and GPUs, making it a valuable tool for understanding their roles in the world of computing. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Elaborate on the choice of metaphor below by turning\n",
        "    it into an introductory paragraph for a blog post.\n",
        "\n",
        "    {student_response}\n",
        "    \"\"\".format(student_response=student_response)\n",
        ")\n",
        "\n",
        "blog_post = response.text\n",
        "\n",
        "print(blog_post)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grdN3VVAQNBC",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1734265075292,
          "user_tz": -330,
          "elapsed": 5042,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "63eb5d0f-bb00-4d9d-b548-e5e29696b202"
      },
      "id": "grdN3VVAQNBC",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Decoding the Performance Puzzle: Race Car vs. SUV - A Dive into TPUs and GPUs\n",
            "\n",
            "In the world of Artificial Intelligence, processing power reigns supreme. But navigating the landscape of processing choices can be as daunting as finding the right vehicle for your needs. Do you opt for the raw speed of a race car, or the versatile practicality of an SUV? This same dilemma applies to the choice between Tensor Processing Units (TPUs) and Graphics Processing Units (GPUs). \n",
            "\n",
            "Much like the race car on a specific track, TPUs are built for lightning-fast performance for specific AI tasks. They are the single-minded champions of their domain, delivering unparalleled speed within their specialized area. However, like the race car's lack of adaptability, TPUs struggle to navigate beyond their defined tracks. This is where the adaptable SUV-like nature of GPUs comes in. GPUs, much like their vehicular counterparts, offer greater versatility. They can handle diverse computational needs, making them adaptable to various applications beyond AI. This versatility, however, comes at the cost of raw speed. On specialized AI tasks, GPUs may not match the blistering performance of their race car equivalent.\n",
            "\n",
            "The race car vs. SUV metaphor provides a clear picture of the trade-off between specialization and versatility. While the race car, though incredibly fast, is impractical for daily driving, the SUV, though slower on the track, offers the flexibility to navigate diverse terrains. Similarly, selecting between a TPU and a GPU hinges on the specific task at hand. If your focus is on achieving the highest performance for a specific AI task, a TPU is the undisputed champion. However, for broader tasks requiring greater versatility across various computational needs, a GPU might be your more reliable off-road companion. \n",
            "\n",
            "This race car vs. SUV metaphor encapsulates the strengths and limitations of both TPUs and GPUs, serving as a valuable tool for deciphering the performance puzzle in the world of computing. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dr0GOBkCQZP1"
      },
      "id": "dr0GOBkCQZP1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "prompt-design-best-practices.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}