{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# [TODO] Add your H1 title heading here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.10.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "{TODO: Include a paragraph or two explaining what this example demonstrates, who should be interested in it, and what you need to know before you get started.}\n",
    "\n",
    "Learn more about [web-doc-title](linkback-to-webdoc-page). {TODO: if more than one primary feature, add tag/linkback for each one}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to {TODO: Complete the sentence explaining briefly what you will learn from the notebook, such as\n",
    "training, hyperparameter tuning, or serving}:\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- *{TODO: Add high level bullets for the services/resources demonstrated; e.g., Vertex AI Training}*\n",
    "\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- *{TODO: Add high level bullets for the steps of performed in the notebook}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "{TODO: Include a paragraph with Dataset information and where to obtain it.} \n",
    "\n",
    "{TODO: Make sure the dataset is accessible to the public. **Googlers**: Add your dataset to the [public samples bucket](http://goto/cloudsamples#sample-storage-bucket) within gs://cloud-samples-data/vertex-ai, if it doesn't already exist there.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs \n",
    "\n",
    "{TODO: Update the list of billable products that your tutorial uses.}\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* {TODO: BigQuery}\n",
    "* Cloud Storage\n",
    "\n",
    "{TODO: Include links to pricing documentation for each product you listed above.\n",
    " NOTE: If you use BigQuery or Dataflow, you need to add this to the pricing.\n",
    "}\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "{ TODO: [BigQuery pricing](https://cloud.google.com/bigquery/pricing), }\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), \n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n",
    "\n",
    "{TODO: Suggest using the latest major GA version of each package; i.e., --upgrade}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "The Cloud SDK, code and other libraries currently run as the service account identity of the Workbench Instance running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "**- Authenticate the Cloud SDK with your credentials :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "authlibtitle"
   },
   "source": [
    "**- Authenticate code and libraries with your credentials :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "authlibcode"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth application-default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "**- Service account or other**\n",
    "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets.\n",
    "\n",
    "- *{Note to notebook author: For any user-provided strings that need to be unique (like bucket names or model ID's), append \"-unique\" to the end so proper testing can occur}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "{TODO: Include commands to delete individual resources below}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete endpoint resource\n",
    "# e.g. `endpoint.delete()`\n",
    "\n",
    "# Delete model resource\n",
    "# e.g. `model.delete()`\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What you will learn:\n",
    "Query a public dataset using BigQuery.\n",
    "Convert a BigQuery dataset to embeddings using the Vertex AI Text-Embeddings API.\n",
    "Create an index in Vertex AI Vector Search.\n",
    "Upload embeddings to the index.\n",
    "Create an index endpoint.\n",
    "Deploy the index to the index endpoint.\n",
    "Perform an online query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first cell, run the following command to install the Google Cloud Vertex AI, Cloud Storage and BigQuery SDKs. To run the command, execute SHIFT+ENTER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in ./.local/lib/python3.10/site-packages (1.75.0)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (2.14.0)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery[pandas] in ./.local/lib/python3.10/site-packages (3.27.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in ./.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.24.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.36.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.13.1)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.9.2)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (15.0.2)\n",
      "Requirement already satisfied: db-dtypes<2.0.0dev,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery[pandas]) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from db-dtypes<2.0.0dev,>=0.3.0->google-cloud-bigquery[pandas]) (1.26.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.68.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.0->google-cloud-bigquery[pandas]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.0->google-cloud-bigquery[pandas]) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery[pandas]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.8.30)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
      "Downloading google_cloud_storage-2.19.0-py2.py3-none-any.whl (131 kB)\n",
      "Installing collected packages: google-cloud-storage\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.14.0\n",
      "    Uninstalling google-cloud-storage-2.14.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.14.0\n",
      "Successfully installed google-cloud-storage-2.19.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        'google-cloud-bigquery[pandas]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernal\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting up the enivironment values for the project\n",
    "\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-east1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project = PROJECT_ID,\n",
    "              location = REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Prepare the data in BigQuery\n",
    "The dataset used for this lab is the StackOverflow dataset. This public dataset is hosted in Google BigQuery and is included in BigQuery's 1TB/mo of free tier processing. This means that each user receives 1TB of free BigQuery processing every month, which can be used to run queries on this public dataset.\n",
    "\n",
    "Stack Overflow is the largest online community for programmers to learn, share their knowledge, and advance their careers. Updated on a quarterly basis, this BigQuery dataset includes an archive of Stack Overflow content, including posts, votes, tags, and badges. This dataset is updated to mirror the Stack Overflow content on the Internet Archive, and is also available through the Stack Exchange Data Explorer.\n",
    "\n",
    "The BigQuery table is too large to fit into memory, so you need to write a generator called query_bigquery_chunks to yield chunks of the dataframe for processing. Additionally, an extra column title_with_body is added, which is a concatenation of the question title and body that will be used for creating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the libraries and initialize the BigQuery client.\n",
    "import math\n",
    "from typing import Any, Generator\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the BigQuery query for the remote dataset.\n",
    "QUERY_TEMPLATE = \"\"\"\n",
    "        SELECT distinct q.id, q.title, q.body\n",
    "        FROM (SELECT * FROM `bigquery-public-data.stackoverflow.posts_questions` where Score>0 ORDER BY View_Count desc) AS q\n",
    "        LIMIT {limit} OFFSET {offset};\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a generator function called query_bigquery_chunks that is used to query a BigQuery database in chunks. Here's a breakdown of what the code does:\n",
    "\n",
    "Function signature\n",
    "\n",
    "The function takes three parameters:\n",
    "\n",
    "max_rows: The maximum number of rows to query from the database.\n",
    "rows_per_chunk: The number of rows to query in each chunk.\n",
    "start_chunk: The starting chunk number (default is 0).\n",
    "The function returns a generator that yields pandas DataFrames.\n",
    "\n",
    "Looping over chunks\n",
    "\n",
    "The function uses a for loop to iterate over the chunks of rows to query. The loop starts at start_chunk and increments by rows_per_chunk until it reaches max_rows.\n",
    "\n",
    "Constructing the query\n",
    "\n",
    "Inside the loop, the function constructs a query string using the QUERY_TEMPLATE format string. The query string includes the limit and offset parameters, which are set to rows_per_chunk and offset, respectively.\n",
    "\n",
    "Running the query\n",
    "\n",
    "The function uses the client.query method to run the query and retrieve the results.\n",
    "\n",
    "Processing the results\n",
    "\n",
    "The function processes the results by converting them to a pandas DataFrame using the to_dataframe method. It then adds a new column called title_with_body by concatenating the title and body columns with a newline character (\\n) in between.\n",
    "\n",
    "Yielding the DataFrame\n",
    "\n",
    "Finally, the function yields the processed DataFrame.\n",
    "\n",
    "Example usage\n",
    "\n",
    "Here's an example of how you might use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_bigquery_chunks(\n",
    "    max_rows: int, rows_per_chunk: int, start_chunk: int = 0\n",
    ") -> Generator[pd.DataFrame, Any, None]:\n",
    "    for offset in range(start_chunk, max_rows, rows_per_chunk):\n",
    "        query = QUERY_TEMPLATE.format(limit=rows_per_chunk, offset=offset)\n",
    "        query_job = client.query(query)\n",
    "        rows = query_job.result()\n",
    "        df = rows.to_dataframe()\n",
    "        df[\"title_with_body\"] = df.title + \"\\n\" + df.body\n",
    "        yield df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>title_with_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4255120</td>\n",
       "      <td>How to revert an HttpError ConfigurationElemen...</td>\n",
       "      <td>&lt;p&gt;Suppose I have the following &lt;code&gt;&amp;lt;http...</td>\n",
       "      <td>How to revert an HttpError ConfigurationElemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4421341</td>\n",
       "      <td>Haskell data types usage good practicies</td>\n",
       "      <td>&lt;p&gt;Reading \"Real world Haskell\" i found some i...</td>\n",
       "      <td>Haskell data types usage good practicies\\n&lt;p&gt;R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4492944</td>\n",
       "      <td>Extract base url from full url</td>\n",
       "      <td>&lt;p&gt;Any quick way to extract base url from full...</td>\n",
       "      <td>Extract base url from full url\\n&lt;p&gt;Any quick w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4473573</td>\n",
       "      <td>Android Landscape mode cutting off my buttons ...</td>\n",
       "      <td>&lt;p&gt;I am having an issue with my application. I...</td>\n",
       "      <td>Android Landscape mode cutting off my buttons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4588542</td>\n",
       "      <td>Sorting NSArray which many contain number</td>\n",
       "      <td>&lt;p&gt;I have an NSArray which is populated with o...</td>\n",
       "      <td>Sorting NSArray which many contain number\\n&lt;p&gt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  4255120  How to revert an HttpError ConfigurationElemen...   \n",
       "1  4421341           Haskell data types usage good practicies   \n",
       "2  4492944                     Extract base url from full url   \n",
       "3  4473573  Android Landscape mode cutting off my buttons ...   \n",
       "4  4588542          Sorting NSArray which many contain number   \n",
       "\n",
       "                                                body  \\\n",
       "0  <p>Suppose I have the following <code>&lt;http...   \n",
       "1  <p>Reading \"Real world Haskell\" i found some i...   \n",
       "2  <p>Any quick way to extract base url from full...   \n",
       "3  <p>I am having an issue with my application. I...   \n",
       "4  <p>I have an NSArray which is populated with o...   \n",
       "\n",
       "                                     title_with_body  \n",
       "0  How to revert an HttpError ConfigurationElemen...  \n",
       "1  Haskell data types usage good practicies\\n<p>R...  \n",
       "2  Extract base url from full url\\n<p>Any quick w...  \n",
       "3  Android Landscape mode cutting off my buttons ...  \n",
       "4  Sorting NSArray which many contain number\\n<p>...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = next(query_bigquery_chunks(max_rows=1000, rows_per_chunk=1000))\n",
    "\n",
    "# Examine the data\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to revert an HttpError ConfigurationElement back to \"Inherited\" using IIS 7 API'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Suppose I have the following <code>&lt;httpErrors&gt;</code> collection in a <code>web.config</code>:</p>\\n\\n<pre><code>&lt;httpErrors&gt;\\n&lt;/httpErrors&gt;\\n</code></pre>\\n\\n<p>Yep, nice \\'n empty.</p>\\n\\n<p>And in IIS 7, my HTTP errors page looks like this:</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/EBcgJ.png\" alt=\"httpErrors\"></p>\\n\\n<p>Beautiful! (I\\'ve highlighted 404 simply because that\\'s the example I\\'ll use in a sec).</p>\\n\\n<p>Now, I run the following code:</p>\\n\\n<pre><code>errorElement[\"statusCode\"] = 404;\\nerrorElement[\"subStatusCode\"] = -1;\\nerrorElement[\"path\"] = \"/404.html\";\\nhttpErrorsCollection.Add(errorElement);\\n</code></pre>\\n\\n<p>Lovely. I now have, as expected this in my <code>web.config</code>:</p>\\n\\n<pre><code>&lt;httpErrors&gt;\\n    &lt;remove statusCode=\"404\" subStatusCode=\"-1\" /&gt;\\n    &lt;error statusCode=\"404\" subStatusCode=\"-1\" prefixLanguageFilePath=\"\" path=\"/404.html\" /&gt;\\n&lt;/httpErrors&gt;\\n</code></pre>\\n\\n<p>Couldn\\'t be happier. Now, in IIS 7 my HTTP errors section looks, as expected, like below:\\n<img src=\"https://i.stack.imgur.com/Z3utD.png\" alt=\"alt text\"></p>\\n\\n<p>Life couldn\\'t be sweeter at this point. Now, down the line, I want to programmatically revert my 404 error back to the state shown in the original screenshot. Logic dictates that I should <code>remove</code> my new error:</p>\\n\\n<pre><code>httpErrorsCollection.Remove(errorElement);\\n</code></pre>\\n\\n<p>But alas, if I do this, my <code>web.config</code> looks a lot like this:</p>\\n\\n<pre><code>    &lt;httpErrors&gt;\\n        &lt;remove statusCode=\"404\" subStatusCode=\"-1\" /&gt;\\n    &lt;/httpErrors&gt;\\n</code></pre>\\n\\n<p>And my IIS looks a bit like this:</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/TsxQg.png\" alt=\"alt text\"></p>\\n\\n<p>This is expected because of my <code>web.config</code> - but how, using <code>ServerManager</code> and all it\\'s useful IIS 7 API, do I remove the <code>httpError</code> element entirely and revert my <code>web.config</code> back to:</p>\\n\\n<pre><code>&lt;httpErrors&gt;\\n&lt;/httpErrors&gt;\\n</code></pre>\\n\\n<p>Any ideas?</p>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['body'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Create text embeddings from BigQuery data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function called encode_texts_to_embeddings that takes a list of sentences as input and returns a list of embeddings for each sentence. Here's a breakdown of what the code does:\n",
    "\n",
    "Function signature\n",
    "\n",
    "The function takes one parameter:\n",
    "\n",
    "sentences: A list of strings, where each string is a sentence.\n",
    "The function returns a list of optional lists of floats, where each inner list represents the embedding for a sentence.\n",
    "\n",
    "Try-except block\n",
    "\n",
    "The function uses a try-except block to handle any exceptions that may occur during the execution of the code.\n",
    "\n",
    "Getting embeddings\n",
    "\n",
    "Inside the try block, the function calls the get_embeddings method of the model object, passing in the list of sentences as input. This method is expected to return a list of embeddings, where each embedding is a vector representation of a sentence.\n",
    "\n",
    "Processing embeddings\n",
    "\n",
    "The function then processes the embeddings by extracting the values from each embedding object and storing them in a list. This is done using a list comprehension: [embedding.values for embedding in embeddings].\n",
    "\n",
    "Returning embeddings\n",
    "\n",
    "The function returns the list of embeddings.\n",
    "\n",
    "Handling exceptions\n",
    "\n",
    "If an exception occurs during the execution of the code, the function catches the exception and returns a list of None values, where each None value corresponds to a sentence in the input list. This is done using a list comprehension: [None for _ in range(len(sentences))]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an embedding method that uses the model.\n",
    "def encode_texts_to_embeddings(sentences: List[str]) -> List[Optional[List[float]]]:\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, each request can handle up to 5 text instances. So we will need to split the BigQuery question results in batches of 5 before sending to the embedding API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generate_batches to split results in batches of 5 to be sent to the embeddings API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "This code defines a generator function called generate_batches that takes a list of sentences and a batch size as input and yields batches of sentences. Here's a breakdown of what the code does:\n",
    "\n",
    "Function signature\n",
    "\n",
    "The function takes two parameters:\n",
    "\n",
    "sentences: A list of strings, where each string is a sentence.\n",
    "batch_size: An integer that specifies the number of sentences to include in each batch.\n",
    "The function returns a generator that yields lists of strings, where each list represents a batch of sentences.\n",
    "\n",
    "Generator function\n",
    "\n",
    "The function uses a generator function to yield batches of sentences. A generator function is a special type of function that can be used to generate a sequence of values on the fly, rather than computing them all at once and storing them in memory.\n",
    "\n",
    "Looping over sentences\n",
    "\n",
    "The function uses a for loop to iterate over the list of sentences in batches of batch_size. The loop starts at index 0 and increments by batch_size each time.\n",
    "\n",
    "Yielding batches\n",
    "\n",
    "Inside the loop, the function yields a batch of sentences using the yield keyword. The batch is created by slicing the list of sentences from the current index i to i + batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Generator, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Generator function to yield batches of sentences\n",
    "def generate_batches(\n",
    "    sentences: List[str], batch_size: int\n",
    ") -> Generator[List[str], None, None]:\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encapsulate the process of generating batches and calling the embeddings API in a method called encode_text_to_embedding_batched. This method also handles rate-limiting using time.sleep. For production use cases, you would want a more sophisticated rate-limiting mechanism that takes retries into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def encode_text_to_embedding_batched(\n",
    "#     sentences: List[str], api_calls_per_second: int = 10, batch_size: int = 5\n",
    "# ) -> Tuple[List[bool], np.ndarray]:\n",
    "\n",
    "#     embeddings_list: List[List[float]] = []\n",
    "\n",
    "#     # Prepare the batches using a generator\n",
    "#     batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "#     seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = []\n",
    "#         for batch in tqdm(\n",
    "#             batches, total=math.ceil(len(sentences) / batch_size), position=0\n",
    "#         ):\n",
    "#             futures.append(\n",
    "#                 executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "#             )\n",
    "#             time.sleep(seconds_per_job)\n",
    "\n",
    "#         for future in futures:\n",
    "#             embeddings_list.extend(future.result())\n",
    "\n",
    "#     is_successful = [\n",
    "#         embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "#     ]\n",
    "#     embeddings_list_successful = np.squeeze(\n",
    "#         np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "#     )\n",
    "#     return is_successful, embeddings_list_successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the encoding function by encoding a subset of data and see if the embeddings and distance metrics make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_text_to_embedding_batched(\n",
    "    sentences: List[str], api_calls_per_second: int = 10, batch_size: int = 5\n",
    ") -> Tuple[List[bool], np.ndarray]:\n",
    "\n",
    "    embeddings_list: List[List[float]] = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total=math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(executor.submit(encode_texts_to_embeddings, batch))\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [embedding is not None for embedding in embeddings_list]\n",
    "    embeddings_list_successful = np.array(\n",
    "        [embedding for embedding in embeddings_list if embedding is not None]\n",
    "    )\n",
    "    return is_successful, embeddings_list_successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9896d89f7f1b4fa9b47ea51b0fef3407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode a subset of questions for validation\n",
    "questions = df.title.tolist()[:500]\n",
    "is_successful, question_embeddings = encode_text_to_embedding_batched(\n",
    "    sentences=df.title.tolist()[:500]\n",
    ")\n",
    "\n",
    "# Filter for successfully embedded sentences\n",
    "questions = np.array(questions)[is_successful]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dimension size for later usage when creating the Vertex AI Vector Search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "DIMENSIONS = len(question_embeddings[0])\n",
    "\n",
    "print(DIMENSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort questions in order of similarity. According to the embedding documentation, the similarity of embeddings is calculated using the dot-product, with np.dot. Once you have the similarity score, sort the results and print them for inspection. 1 means very similar, 0 means very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query question = Finding All Combinations (Cartesian product) of JavaScript array values\n",
      "\t0: Finding All Combinations (Cartesian product) of JavaScript array values: 0.9999977794032682\n",
      "\t1: Javascript, filter object list using string condition?: 0.5625301903976244\n",
      "\t2: Match backwards from a given word with javascript: 0.5504823359661402\n",
      "\t3: javascript chinese/japanese character decoding: 0.5147855421339054\n",
      "\t4: PHP recursion: How to create a recursion in php: 0.4875655962986394\n",
      "\t5: How can I populate a select dropdown list from a JSON feed with AngularJS?: 0.4775828568939623\n",
      "\t6: how to extract this array through foreach loop: 0.47443788106366486\n",
      "\t7: scala: accumulate a var from collection in a functional manner (that is, no vars): 0.47109930942906036\n",
      "\t8: jQuery & document.write: 0.46812234735227976\n",
      "\t9: Difference between floats and ints in Javascript?: 0.4484692176295221\n",
      "\t10: How to execute JS from Python, that uses 'Document' and/or 'Window': 0.4429945344773859\n",
      "\t11: How to Convert an ArrayList to string C#: 0.44194079396647645\n",
      "\t12: Sorting NSArray which many contain number: 0.43879738299503634\n",
      "\t13: Recursively find all files that match a certain pattern: 0.433762403181813\n",
      "\t14: How to rewrite this .Net C# in ColdFusion?: 0.43035032516895105\n",
      "\t15: Python list + list vs. list.append(): 0.4238483465544336\n",
      "\t16: Force Users to Click <a> Links In Sequential <ol> <li> Order: 0.42292952440943055\n",
      "\t17: Beginner python set intersection error: 0.4205815656971253\n",
      "\t18: Create matrix with same in and out degree for all nodes: 0.4196843911536925\n",
      "\t19: How can I resize a 2D vector of objects given the width and height?: 0.41273385457533135\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "question_index = random.randint(0, 99)\n",
    "\n",
    "print(f\"Query question = {questions[question_index]}\")\n",
    "\n",
    "# Get similarity scores for each embedding by using dot-product.\n",
    "scores = np.dot(question_embeddings[question_index], question_embeddings.T)\n",
    "\n",
    "# Print top 20 matches\n",
    "for index, (question, score) in enumerate(\n",
    "    sorted(zip(questions, scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "):\n",
    "    print(f\"\\t{index}: {question}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save the embeddings in JSONL format. The data must be formatted in JSONL format, which means each embedding dictionary is written as an individual JSON object on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings directory: /var/tmp/tmp9fm69ik5\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary file to write embeddings to\n",
    "embeddings_file_path = Path(tempfile.mkdtemp())\n",
    "\n",
    "print(f\"Embeddings directory: {embeddings_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write embeddings in batches to prevent out-of-memory errors. Notice we are only using 5000 questions so that the embedding creation process and indexing is faster. The dataset contains more than 50,000 questions. This step will take around 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is designed to process a large dataset from BigQuery in chunks, convert each chunk to embeddings using an API, and save the embeddings to a file. Here's a breakdown of what the code does:\n",
    "\n",
    "**Constants and variables**\n",
    "\n",
    "The code defines several constants and variables:\n",
    "\n",
    "* `BQ_NUM_ROWS`: The total number of rows to process from BigQuery.\n",
    "* `BQ_CHUNK_SIZE`: The number of rows to process in each chunk.\n",
    "* `BQ_NUM_CHUNKS`: The total number of chunks to process, calculated by dividing `BQ_NUM_ROWS` by `BQ_CHUNK_SIZE`.\n",
    "* `START_CHUNK`: The starting chunk number, set to 0.\n",
    "* `API_CALLS_PER_SECOND`: The rate limit for API calls, set to 300 requests per minute.\n",
    "* `ITEMS_PER_REQUEST`: The number of items that can be processed per API request, set to 5.\n",
    "\n",
    "**Looping through chunks**\n",
    "\n",
    "The code uses a `for` loop to iterate through each chunk of data from BigQuery. The loop uses the `query_bigquery_chunks` function to generate each chunk, and the `tqdm` library to display a progress bar.\n",
    "\n",
    "**Processing each chunk**\n",
    "\n",
    "Inside the loop, the code processes each chunk as follows:\n",
    "\n",
    "1. Creates a unique output file for each chunk using the `embeddings_file_path` variable.\n",
    "2. Extracts the `id` column from the chunk using the `id_chunk` variable.\n",
    "3. Converts the chunk to embeddings using the `encode_text_to_embedding_batched` function, passing in the `sentences` column from the chunk, the `api_calls_per_second` rate limit, and the `batch_size` of 5 items per request.\n",
    "4. Formats the embeddings as a list of JSON objects, where each object contains the `id` and `embedding` values.\n",
    "5. Appends the formatted embeddings to the output file using the `writelines` method.\n",
    "6. Deletes the chunk DataFrame and any other large data structures using the `del` statement and the `gc.collect` function to free up memory.\n",
    "\n",
    "**Saving embeddings to file**\n",
    "\n",
    "The code saves the embeddings to a file in JSON format, where each line represents a single embedding. The file is created in the same directory as the `embeddings_file_path` variable, with a unique name for each chunk.\n",
    "\n",
    "Overall, this code is designed to process a large dataset from BigQuery in chunks, convert each chunk to embeddings using an API, and save the embeddings to a file in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c610660757b94e3e91dd0d1f3576d350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk of rows from BigQuery:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7de558058c420886ba4b6c24007fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51548e6799c4eddae3fdc1c0e45ce94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee93bc7791c483d9eed355c4e702570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66042bd2364b4375986de37dfee1dbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea35c43953d49debde93cc2fa7b64cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "\n",
    "BQ_NUM_ROWS = 5000\n",
    "BQ_CHUNK_SIZE = 1000\n",
    "BQ_NUM_CHUNKS = math.ceil(BQ_NUM_ROWS / BQ_CHUNK_SIZE)\n",
    "\n",
    "START_CHUNK = 0\n",
    "\n",
    "# Create a rate limit of 300 requests per minute. Adjust this depending on your quota.\n",
    "API_CALLS_PER_SECOND = 300 / 60\n",
    "# According to the docs, each request can process 5 instances per request\n",
    "ITEMS_PER_REQUEST = 5\n",
    "\n",
    "# Loop through each generated dataframe, convert\n",
    "for i, df in tqdm(\n",
    "    enumerate(\n",
    "        query_bigquery_chunks(\n",
    "            max_rows=BQ_NUM_ROWS, rows_per_chunk=BQ_CHUNK_SIZE, start_chunk=START_CHUNK\n",
    "        )\n",
    "    ),\n",
    "    total=BQ_NUM_CHUNKS - START_CHUNK,\n",
    "    position=-1,\n",
    "    desc=\"Chunk of rows from BigQuery\",\n",
    "):\n",
    "    # Create a unique output file for each chunk\n",
    "    chunk_path = embeddings_file_path.joinpath(\n",
    "        f\"{embeddings_file_path.stem}_{i+START_CHUNK}.json\"\n",
    "    )\n",
    "    with open(chunk_path, \"a\") as f:\n",
    "        id_chunk = df.id\n",
    "\n",
    "        # Convert batch to embeddings\n",
    "        is_successful, question_chunk_embeddings = encode_text_to_embedding_batched(\n",
    "            sentences=df.title_with_body.to_list(),\n",
    "            api_calls_per_second=API_CALLS_PER_SECOND,\n",
    "            batch_size=ITEMS_PER_REQUEST,\n",
    "        )\n",
    "\n",
    "        # Append to file\n",
    "        embeddings_formatted = [\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"id\": str(id),\n",
    "                    \"embedding\": [str(value) for value in embedding],\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "            for id, embedding in zip(id_chunk[is_successful], question_chunk_embeddings)\n",
    "        ]\n",
    "        f.writelines(embeddings_formatted)\n",
    "\n",
    "        # Delete the DataFrame and any other large data structures\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Upload embeddings to Cloud Storage\n",
    "Upload the text-embeddings to Cloud Storage, so that Vertex AI Vector Search can access them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a bucket where you will store your embeddings.\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-01-aac69c1b7459-unique/...\n"
     ]
    }
   ],
   "source": [
    "#Create your Cloud Storage bucket.\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///var/tmp/tmp9fm69ik5/tmp9fm69ik5_1.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmp9fm69ik5/tmp9fm69ik5_0.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmp9fm69ik5/tmp9fm69ik5_3.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmp9fm69ik5/tmp9fm69ik5_2.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmp9fm69ik5/tmp9fm69ik5_4.json [Content-Type=application/json]...\n",
      "/ [5/5 files][ 12.3 MiB/ 12.3 MiB] 100% Done                                    \n",
      "Operation completed over 5 objects/12.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Upload the training data to a Google Cloud Storage bucket.\n",
    "remote_folder = f\"{BUCKET_URI}/{embeddings_file_path.stem}/\"\n",
    "! gsutil -m cp -r {embeddings_file_path}/* {remote_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7. Create an Index in Vertex AI Vector Search for your embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup your index name and description.\n",
    "DISPLAY_NAME = \"stack_overflow\"\n",
    "DESCRIPTION = \"question titles and bodies from stackoverflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the index. Notice that the index reads the embeddings from the Cloud Storage bucket. The indexing process can take from 45 minutes up to 60 minutes. Wait for completion, and then proceed. You can open a different Google Cloud Console page, navigate to Vertex AI Vector search, and see how the index is being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "DIMENSIONS = 768\n",
    "\n",
    "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    contents_delta_uri=remote_folder,\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=80,\n",
    "    description=DESCRIPTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reference the index name to make sure it got created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/80366348363/locations/us-east1/indexes/2554007181649248256'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
    "INDEX_RESOURCE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Using the resource name, you can retrieve an existing MatchingEngineIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_ah_index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an IndexEndpoint so that it can be accessed via an API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/80366348363/locations/us-east1/indexEndpoints/5249816178909511680/operations/6460950914332098560\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/80366348363/locations/us-east1/indexEndpoints/5249816178909511680\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/80366348363/locations/us-east1/indexEndpoints/5249816178909511680')\n"
     ]
    }
   ],
   "source": [
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    description=DISPLAY_NAME,\n",
    "    public_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/80366348363/locations/us-east1/indexEndpoints/5249816178909511680\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/80366348363/locations/us-east1/indexEndpoints/5249816178909511680/operations/1214257348445470720\n"
     ]
    }
   ],
   "source": [
    "# Deploy your index to the created endpoint. This can take up to 15 minutes.\n",
    "DEPLOYED_INDEX_ID = \"deployed_index_id_unique\"\n",
    "\n",
    "DEPLOYED_INDEX_ID\n",
    "\n",
    "\n",
    "my_index_endpoint = my_index_endpoint.deploy_index(\n",
    "    index=tree_ah_index, deployed_index_id=DEPLOYED_INDEX_ID\n",
    ")\n",
    "\n",
    "my_index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify number of declared items matches the number of embeddings. Each IndexEndpoint can have multiple indexes deployed to it. For each index, you can retrieve the number of deployed vectors using the index_endpoint._gca_resource.index_stats.vectors_count. The numbers may not match exactly due to potential rate-limiting failures incurred when using the embedding service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_vectors = sum(\n",
    "    aiplatform.MatchingEngineIndex(\n",
    "        deployed_index.index\n",
    "    )._gca_resource.index_stats.vectors_count\n",
    "    for deployed_index in my_index_endpoint.deployed_indexes\n",
    ")\n",
    "\n",
    "print(f\"Expected: {BQ_NUM_ROWS}, Actual: {number_of_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8. Create online queries\n",
    "After you build your indexes, you may query against the deployed index to find nearest neighbors.\n",
    "\n",
    "Note: For the DOT_PRODUCT_DISTANCE distance type, the \"distance\" property returned with each MatchNeighbor actually refers to the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an embedding for a test question.\n",
    "test_embeddings = encode_texts_to_embeddings(sentences=[\"Install GPU for Tensorflow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the query to retrieve the similar embeddings.\n",
    "NUM_NEIGHBOURS = 10\n",
    "\n",
    "response = my_index_endpoint.find_neighbors(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    queries=test_embeddings,\n",
    "    num_neighbors=NUM_NEIGHBOURS,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify that the retrieved results are relevant by checking the StackOverflow links.\n",
    "for match_index, neighbor in enumerate(response[0]):\n",
    "    print(f\"https://stackoverflow.com/questions/{neighbor.id}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
